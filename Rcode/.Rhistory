#0 being the majority class
result_df_test$Baseline_Brier_Score <- (result_df_test$Actual - 0)^2
result_df_test
reliability.plot(verify(result_df_test$Actual, result_df_test$s0, nbins=20))
cat('Brier Score: ', mean(result_df_test$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df_test$Baseline_Brier_Score))
y_test
mean(y_test)
mean(y)
model
coef(glmnet)
coef(model)
coef(model)[,'s0']
coef(model)[,'s0'] == 0
sum(coef(model)[,'s0'] == 0)
sum(coef(model)[,'s0'] <= 0.001)
sum(coef(model)[,'s0'] == 0.0)
cat(sum(coef(model)[,'s0'] == 0.0), ' features were dropped')
coef(model)[,'s0']
sort(coef(model)[,'s0'])
sort(abs(coef(model)[,'s0']))
reliability_diagram <- function(y_true, y_prob, n_bins = 10) {
# Create bins for predicted probabilities
bins <- seq(0, 1, length.out = n_bins + 1)
# Calculate mean predicted probability and observed frequency in each bin
bin_centers <- (bins[-1] + bins[-length(bins)]) / 2
mean_prob <- numeric(n_bins)
obs_freq <- numeric(n_bins)
for (i in seq_along(bins)[-length(bins)]) {
bin_indices <- y_prob >= bins[i] & y_prob < bins[i + 1]
mean_prob[i] <- mean(y_prob[bin_indices])
obs_freq[i] <- mean(y_true[bin_indices])
print(sum(bin_indices))
}
# Plot the reliability diagram
# plot(bin_centers, mean_prob, type = "o", col = "blue", pch = 16,
#      ylim = c(0, 1), xlim = c(0, 1), xlab = "Mean Predicted Probability",
#      ylab = "Observed Frequency")
# Add points representing observed frequencies
plot(bin_centers, obs_freq, col = "black", pch = 1, ylim = c(0, 1), xlim = c(0, 1), xlab = "Mean Predicted Probability", ylab = "Observed Frequency", lty = 2, type="o")
# Add legend
legend("topleft", legend = "Perfect Calibration", col = "red", lty = 2, cex = 0.8)
# Add the 45-degree line for perfect calibration
abline(a = 0, b = 1, col = "red", lty = 2)
# Add title
title(main = "Reliability Diagram")
}
# Example usage:
# y_true: True class labels (0 or 1)
# y_prob: Predicted probabilities
# n_bins: Number of bins for the reliability diagram
set.seed(123)
y_true <- rbinom(100, 1, 0.3)
y_prob <- runif(100)
reliability_diagram(y_true, y_prob, n_bins = 10)
reliability_diagram <- function(y_true, y_prob, n_bins = 10) {
# Create bins for predicted probabilities
bins <- seq(0, 1, length.out = n_bins + 1)
# Calculate mean predicted probability and observed frequency in each bin
bin_centers <- (bins[-1] + bins[-length(bins)]) / 2
mean_prob <- numeric(n_bins)
obs_freq <- numeric(n_bins)
for (i in seq_along(bins)[-length(bins)]) {
bin_indices <- y_prob >= bins[i] & y_prob < bins[i + 1]
mean_prob[i] <- mean(y_prob[bin_indices])
obs_freq[i] <- mean(y_true[bin_indices])
print(sum(bin_indices))
}
# Plot the reliability diagram
# plot(bin_centers, mean_prob, type = "o", col = "blue", pch = 16,
#      ylim = c(0, 1), xlim = c(0, 1), xlab = "Mean Predicted Probability",
#      ylab = "Observed Frequency")
# Add points representing observed frequencies
plot(bin_centers, obs_freq, col = "black", pch = 1, ylim = c(0, 1), xlim = c(0, 1), xlab = "Mean Predicted Probability", ylab = "Observed Frequency", lty = 2, type="o")
# Add legend
legend("topleft", legend = "Perfect Calibration", col = "red", lty = 2, cex = 0.8)
# Add the 45-degree line for perfect calibration
abline(a = 0, b = 1, col = "red", lty = 2)
# Add title
title(main = "Reliability Diagram")
}
y_true <- result_df_test$Actual
y_prob <- result_df_test$s0
reliability_diagram(y_true, y_prob, n_bins = 10)
final_df <- copy(preprocessed_df_test)
preprocessed_df_test
test
final_df <- copy(test)
library(caret)
library(glmnet)
library(randomForest)
library(verification)
library(rms)
library(zoo)
library(gbm)
library(pROC)
library(dplyr)
final_df <- copy(test)
slice(test,)
final_df <- data.frame(test)
final_df$prob <- predictions_test
data.frame(test)
final_df <- data.frame(test)
final_df$prob <- predictions_test
final_df$expectedRev <- final_df$prob * final_df$Annual_Premium
final_df
final_df
final_df <- data.frame(test)
#final_df$prob <- predictions_test
#final_df$expectedRev <- final_df$prob * final_df$Annual_Premium
final_df
final_df <- data.frame(test)
final_df$prob <- predictions_test
#final_df$expectedRev <- final_df$prob * final_df$Annual_Premium
final_df
final_df <- data.frame(test)
final_df$prob <- 1
#final_df$expectedRev <- final_df$prob * final_df$Annual_Premium
final_df
predictions_test
final_df <- data.frame(test)
final_df$s0 <- predictions_test
#final_df$expectedRev <- final_df$prob * final_df$Annual_Premium
final_df
predictions_test
final_df
final_df <- data.frame(test)
#final_df$s0 <-
#final_df$expectedRev <- final_df$prob * final_df$Annual_Premium
final_df
cbind(final_df, prediction_test)
cbind(final_df, predictions_test)
final_df <- data.frame(test)
final_df <- cbind(final_df, predictions_test)
final_df$expectedRev <- final_df$s0 * final_df$Annual_Premium
final_df
hist(final_df$expectedRev)
hist(final_df$expectedRev, bins=10)
hist(final_df$expectedRev, breaks=10)
hist(final_df$expectedRev, breaks=100)
hist(final_df$expectedRev, breaks=100, xlim=100000)
hist(final_df$expectedRev, breaks=100, xlim=10000)
hist(final_df$expectedRev, breaks=100, xlim=c(0,10000))
hist(final_df$expectedRev, breaks=100, xlim=c(0,100000))
sqrt(predict(model, newdata = X_test, type = "link", se.fit = TRUE)$se.fit^2)
predictions_test <- predict(model, newx = X_test, type = "response")
hist(final_df$expectedRev, breaks=100, xlim=c(0,50000))
sum(final_df$expectedRev)
sum(final_df$expectedRev) / 100
predict(model, newx = X_test, s = best_lambda, type = "link")
predict(model, newx = X_test, s = minimum_lambda, type = "link")
log_odds <- predict(model, newx = X_test, s = minimum_lambda, type = "link")
se <- sqrt(diag(predict(model, newx = X_test, s = best_lambda, type = "link", se.fit = TRUE)$se.fit))
log_odds <- predict(model, newx = X_test, s = minimum_lambda, type = "link")
se <- sqrt(diag(predict(model, newx = X_test, s = minimum_lambda, type = "link", se.fit = TRUE)$se.fit))
log_odds <- predict(model, newx = X_test, s = minimum_lambda, type = "link")
diag(predict(model, newx = X_test, s = minimum_lambda, type = "link", se.fit = TRUE)$se.fit)
log_odds <- predict(model, newx = X_test, s = minimum_lambda, type = "link")
predict(model, newx = X_test, s = minimum_lambda, type = "link", se.fit = TRUE)$se.fit
log_odds <- predict(model, newx = X_test, s = minimum_lambda, type = "link")
predict(model, newx = X_test, s = minimum_lambda, type = "link", se.fit = TRUE)
predict(model, newx = X_test, s = minimum_lambda, type = "confidence")
final_df$Response * final_df$Annual_Premium
sum(final_df$Response * final_df$Annual_Premium) / 100
(4015424 - 3993075) / 3993075
(4015424 - 3993075) / 3993075 * 100
ist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium, breaks = 100, xlim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium, breaks = 100, xlim = c(0, 100000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
1 - 0.09531107 / 0.1637708
1 - 0.09531107 / 0.1637708 * 100
1 - (0.09531107 / 0.1637708) * 100
1 - (0.09531107 / 0.1637708) * 100
1 - (0.09531107 / 0.1637708)
(1 - (0.09531107 / 0.1637708) ) * 100
cat('Our Model performs ',(1 - (0.09531107 / 0.1637708) ) * 100, '% better than the baseline')
install.packages("DescTools")
library(caret)
library(glmnet)
library(randomForest)
library(verification)
library(rms)
library(zoo)
library(gbm)
library(pROC)
library(dplyr)
library(DescTools)
concordance.index(result_df_test$Actual, result_df_test$s0)
(final_df$expectedRev - final_df$Response * final_df$Annual_Premium)^2
sum((final_df$expectedRev - final_df$Response * final_df$Annual_Premium)^2)
sum((final_df$expectedRev - final_df$Response * final_df$Annual_Premium)^2) / dim(final_df)[1]
sqrt(sum((final_df$expectedRev - final_df$Response * final_df$Annual_Premium)^2) / dim(final_df)[1])
sum((final_df$expectedRev - final_df$Response * final_df$Annual_Premium)^2) / dim(final_df)
dim(final_df)
sum((final_df$expectedRev - final_df$Response * final_df$Annual_Premium)^2) / 76430
(final_df$expectedRev - final_df$Response * final_df$Annual_Premium)
sum(final_df$expectedRev - final_df$Response * final_df$Annual_Premium)
sum(final_df$expectedRev - final_df$Response * final_df$Annual_Premium) / 100
sum(abs(final_df$expectedRev - final_df$Response * final_df$Annual_Premium)) / 100
sum((final_df$s0 *log(final_df$Annual_Premium) - final_df$Response * log(final_df$Annual_Premium))^2) / 76430
sum((final_df$s0 *log(final_df$Annual_Premium) - final_df$Response * log(final_df$Annual_Premium))^2)
library(caret)
library(glmnet)
library(randomForest)
library(verification)
library(rms)
library(zoo)
library(gbm)
library(pROC)
library(dplyr)
library(DescTools)
df = read.csv('aug_train.csv')
#Dropping the id column since we already have dataframe indices as their unique ID
df = df[, !names(df) %in% 'id']
head(df)
#Making factors of some categorical variables
df$Region_Code <- as.factor(df$Region_Code)
df$Policy_Sales_Channel <- as.factor(df$Policy_Sales_Channel)
df$Previously_Insured <- as.factor(df$Previously_Insured)
df$Gender <- as.factor(df$Gender)
df$Driving_License <- as.factor(df$Driving_License)
df$Vehicle_Age <- as.factor(df$Vehicle_Age)
df$Vehicle_Damage <- as.factor(df$Vehicle_Damage)
set.seed(1010137629)
#Stratified Split
index <- createDataPartition(df$Response, p = 0.8, list = FALSE)
train <- df[index, ]
test <- df[-index, ]
#dummy_transform <- dummyVars(" ~ .", data = preprocessed_df)
dummy_transform <- dummyVars(" ~ .", data = train)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df <- data.frame(predict(dummy_transform, newdata = train))
preprocessed_df$Annual_Premium <- log(preprocessed_df$Annual_Premium)
y <- preprocessed_df$Response
#y <- as.factor(y)
X <- as.matrix(preprocessed_df[, !names(preprocessed_df) %in% 'Response'])
set.seed(1010137629)
lasso_model <- cv.glmnet(X, y, family = "binomial", alpha = 1, verbose=TRUE)
plot(lasso_model)
#0.0002464987
minimum_lambda <- lasso_model$lambda.min
set.seed(1010137629)
model <- glmnet(X, y, family = "binomial", alpha = 1, verbose=TRUE, lambda = minimum_lambda)
model
cat(sum(coef(model)[,'s0'] == 0.0), ' categories were dropped')
sort(abs(coef(model)[,'s0']))
predictions <- predict(model, newx = X, type = "response")
options(scipen = 999)
result_df <- data.frame(Actual = y, Predictions = predictions)
result_df$Brier_Score <- (result_df$Actual - result_df$s0)^2
#0 being the majority class
result_df$Baseline_Brier_Score <- (result_df$Actual - 0)^2
result_df
cat('Brier Score: ', mean(result_df$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df$Baseline_Brier_Score))
# Create a reliability diagram
reliability.plot(verify(result_df$Actual, result_df$s0, nbins=20))
calibrate.plot(result_df$Actual,result_df$s0)
dummy_transform <- dummyVars(" ~ .", data = test)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df_test <- data.frame(predict(dummy_transform, newdata = test))
preprocessed_df_test$Annual_Premium <- log(preprocessed_df_test$Annual_Premium)
y_test <- preprocessed_df_test$Response
#y <- as.factor(y)
X_test <- as.matrix(preprocessed_df_test[, !names(preprocessed_df_test) %in% 'Response'])
predictions_test <- predict(model, newx = X_test, type = "response")
predict(model, newx = X_test, s = minimum_lambda, type = "confidence")
result_df_test <- data.frame(Actual = y_test, Predictions = predictions_test)
result_df_test$Brier_Score <- (result_df_test$Actual - result_df_test$s0)^2
#0 being the majority class
result_df_test$Baseline_Brier_Score <- (result_df_test$Actual - 0)^2
result_df_test
cat('Brier Score: ', mean(result_df_test$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df_test$Baseline_Brier_Score))
df
cat('Brier Score: ', mean(result_df_test$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df_test$Baseline_Brier_Score))
cat('Our Model performs ',(1 - (0.09531107 / 0.1637708) ) * 100, '% better than the baseline')
y_true <- result_df_test$Actual
y_prob <- result_df_test$s0
reliability_diagram(y_true, y_prob, n_bins = 10)
final_df <- data.frame(test)
final_df <- cbind(final_df, predictions_test)
final_df$expectedRev <- final_df$s0 * final_df$Annual_Premium
hist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000), ylim = c(0, 20000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 20000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
final_df
hist(final_df$Annual_Premium * final_df$Response, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 20000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000), ylim = c(0, 40000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000), ylim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium * final_df$Response, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium * final_df$Response, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 60000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium * final_df$Response, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 70000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium * final_df$Response, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 50000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$expectedRev, breaks = 100, xlim = c(0, 50000), ylim = c(0, 30000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
hist(final_df$Annual_Premium * final_df$Response, breaks = 100, xlim = c(0, 100000),  ylim = c(0, 30000),
main = "Expected Revenue Distribution",
xlab = "Expected Revenue",
ylab = "Frequency")
sum((final_df$s0 *log(final_df$Annual_Premium) - final_df$Response * log(final_df$Annual_Premium))^2)
df
library(caret)
library(glmnet)
library(randomForest)
library(verification)
library(rms)
library(zoo)
library(gbm)
library(pROC)
library(dplyr)
library(pROC)
df = read.csv('aug_train.csv')
#Dropping the id column since we already have dataframe indices as their unique ID
df = df[, !names(df) %in% 'id']
head(df)
#Making factors of some categorical variables
df$Region_Code <- as.factor(df$Region_Code)
df$Policy_Sales_Channel <- as.factor(df$Policy_Sales_Channel)
df$Previously_Insured <- as.factor(df$Previously_Insured)
df$Gender <- as.factor(df$Gender)
df$Driving_License <- as.factor(df$Driving_License)
df$Vehicle_Age <- as.factor(df$Vehicle_Age)
df$Vehicle_Damage <- as.factor(df$Vehicle_Damage)
set.seed(1010137629)
index <- createDataPartition(df$Response, p = 0.8, list = FALSE)
train <- df[index, ]
test <- df[-index, ]
train
#dummy_transform <- dummyVars(" ~ .", data = preprocessed_df)
dummy_transform <- dummyVars(" ~ .", data = train)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df <- data.frame(predict(dummy_transform, newdata = train))
#Dropping features to avoid Variable Trap
# preprocessed_df = preprocessed_df[, !names(preprocessed_df) %in% c('GenderMale', 'Vehicle_Age_morethan2', 'Vehicle_DamageYes', 'Region_Code.0', 'Policy_Sales_Channel.1')]
y <- preprocessed_df$Response
#y <- as.factor(y)
X <- as.matrix(preprocessed_df[, !names(preprocessed_df) %in% 'Response'])
set.seed(1010137629)
rf_model <- randomForest(train, as.factor(y), ntree = 30, do.trace=TRUE)
#dummy_transform <- dummyVars(" ~ .", data = preprocessed_df)
dummy_transform <- dummyVars(" ~ .", data = train)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df <- data.frame(predict(dummy_transform, newdata = train))
y <- preprocessed_df$Response
#y <- as.factor(y)
X <- as.matrix(preprocessed_df[, !names(preprocessed_df) %in% 'Response'])
set.seed(1010137629)
rf_model <- randomForest(train, as.factor(y), ntree = 30, do.trace=TRUE)
set.seed(1010137629)
rf_model <- randomForest(X, as.factor(y), ntree = 30, do.trace=TRUE)
predictions <- predict(rf_model, newx = X, type = "prob")
#predictions <- na.fill(as.matrix(predictions), fill=0)
predictions <- predictions[,2]
options(scipen = 999)
result_df <- data.frame(Actual = y, Predictions = predictions)
#result_df$Actual <- as.numeric(result_df$Actual)
#result_df$Predictions <- as.numeric(result_df$Predictions)
result_df$Brier_Score <- (result_df$Actual - result_df$Predictions)^2
#0 being the majority class
result_df$Baseline_Brier_Score <- (result_df$Actual - 0)^2
result_df
cat('Brier Score: ', mean(result_df$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df$Baseline_Brier_Score))
# Create a reliability diagram
reliability.plot(verify(result_df$Actual, result_df$Predictions))
calibrate.plot(result_df$Actual,result_df$Predictions, rug = 10)
dummy_transform <- dummyVars(" ~ .", data = test)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df_test <- data.frame(predict(dummy_transform, newdata = test))
y_test <- preprocessed_df_test$Response
#y <- as.factor(y)
X_test <- as.matrix(preprocessed_df_test[, !names(preprocessed_df_test) %in% 'Response'])
predictions_test <- predict(rf_model,X_test, type = "prob")[,2]
result_df_test <- data.frame(Actual = y_test, Predictions = predictions_test)
#result_df$Actual <- as.numeric(result_df$Actual)
#result_df$Predictions <- as.numeric(result_df$Predictions)
result_df_test$Brier_Score <- (result_df_test$Actual - result_df_test$Predictions)^2
#0 being the majority class
result_df_test$Baseline_Brier_Score <- (result_df_test$Actual - 0)^2
result_df_test
reliability.plot(verify(result_df_test$Actual, result_df_test$Predictions, nbins=20))
library(randomForest)
library(caret)
library(glmnet)
library(randomForest)
library(verification)
library(rms)
library(zoo)
library(gbm)
library(pROC)
library(dplyr)
library(pROC)
df = read.csv('aug_train.csv')
#Dropping the id column since we already have dataframe indices as their unique ID
df = df[, !names(df) %in% 'id']
head(df)
#Making factors of some categorical variables
df$Region_Code <- as.factor(df$Region_Code)
df$Policy_Sales_Channel <- as.factor(df$Policy_Sales_Channel)
df$Previously_Insured <- as.factor(df$Previously_Insured)
df$Gender <- as.factor(df$Gender)
df$Driving_License <- as.factor(df$Driving_License)
df$Vehicle_Age <- as.factor(df$Vehicle_Age)
df$Vehicle_Damage <- as.factor(df$Vehicle_Damage)
set.seed(1010137629)
index <- createDataPartition(df$Response, p = 0.8, list = FALSE)
train <- df[index, ]
test <- df[-index, ]
#dummy_transform <- dummyVars(" ~ .", data = preprocessed_df)
dummy_transform <- dummyVars(" ~ .", data = train)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df <- data.frame(predict(dummy_transform, newdata = train))
#Dropping features to avoid Variable Trap
# preprocessed_df = preprocessed_df[, !names(preprocessed_df) %in% c('GenderMale', 'Vehicle_Age_morethan2', 'Vehicle_DamageYes', 'Region_Code.0', 'Policy_Sales_Channel.1')]
y <- preprocessed_df$Response
#y <- as.factor(y)
X <- as.matrix(preprocessed_df[, !names(preprocessed_df) %in% 'Response'])
set.seed(1010137629)
rf_model <- randomForest(X, as.factor(y), ntree = 30, do.trace=TRUE)
predictions <- predict(rf_model, newx = X, type = "prob")
#predictions <- na.fill(as.matrix(predictions), fill=0)
predictions <- predictions[,2]
options(scipen = 999)
result_df <- data.frame(Actual = y, Predictions = predictions)
#result_df$Actual <- as.numeric(result_df$Actual)
#result_df$Predictions <- as.numeric(result_df$Predictions)
result_df$Brier_Score <- (result_df$Actual - result_df$Predictions)^2
#0 being the majority class
result_df$Baseline_Brier_Score <- (result_df$Actual - 0)^2
result_df
cat('Brier Score: ', mean(result_df$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df$Baseline_Brier_Score))
# Create a reliability diagram
reliability.plot(verify(result_df$Actual, result_df$Predictions))
library(pROC)
a <- result_df
a$Actual <- as.numeric(a$Actual)
roc(a$Actual, a$Prediction)
roc_curve_obj <- roc(a$Actual, a$Prediction, direction = "auto")
auc_pr <- auc(roc_curve_obj, curve = TRUE)
plot(roc_curve_obj, col = "blue", main = "Precision-Recall Curve")
dummy_transform <- dummyVars(" ~ .", data = test)
# Apply the dummy transformation to the data
#preprocessed_df <- data.frame(predict(dummy_transform, newdata = preprocessed_df))
preprocessed_df_test <- data.frame(predict(dummy_transform, newdata = test))
y_test <- preprocessed_df_test$Response
#y <- as.factor(y)
X_test <- as.matrix(preprocessed_df_test[, !names(preprocessed_df_test) %in% 'Response'])
predictions_test <- predict(rf_model,X_test, type = "prob")[,2]
result_df_test <- data.frame(Actual = y_test, Predictions = predictions_test)
#result_df$Actual <- as.numeric(result_df$Actual)
#result_df$Predictions <- as.numeric(result_df$Predictions)
result_df_test$Brier_Score <- (result_df_test$Actual - result_df_test$Predictions)^2
#0 being the majority class
result_df_test$Baseline_Brier_Score <- (result_df_test$Actual - 0)^2
result_df_test
reliability.plot(verify(result_df_test$Actual, result_df_test$Predictions, nbins=20))
cat('Brier Score: ', mean(result_df_test$Brier_Score), '\n', 'Baseline Brier Score: ',
mean(result_df_test$Baseline_Brier_Score))
